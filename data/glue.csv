model,corpus_of_linguistic_acceptability,stanford_sentiment_treebank2,microsoft_research_paraphrase_corpus,semantic_textual_similarity_benchmark,quora_question_pairs,multi_nli_matched,multi_nli_mismatched,question_nli,recognizing_textual_entailment,winograd_nli,AX
Turing ULR v6,73.3,97.5,92.3,93.1,90.9,92.5,92.1,96.7,93.6,97.9,55.4
Vega v1,73.8,97.9,92.6,93.1,91.1,92.1,91.9,96.7,92.4,97.9,51.4
Turing NLR v5 ,72.6,97.6,91.7,93.3,91.1,92.6,92.4,97.9,94.1,95.9,57.0
DeBERTa + CLEVER,74.7,97.6,91.1,93.1,91.0,92.1,91.8,96.7,93.2,96.6,53.3
ERNIE,75.5,97.8,91.8,92.6,90.9,92.3,91.7,97.3,92.6,95.9,51.7
StructBERT + CLEVER,75.3,97.7,91.9,93.1,90.8,91.7,91.5,97.4,92.5,95.2,49.1
DeBERTa / TuringNLRv4,71.5,97.5,92.0,92.6,90.8,91.9,91.6,99.2,93.2,94.5,53.2
MacALBERT + DKM,74.8,97.0,92.6,92.6,90.6,91.3,91.1,97.8,92.0,94.5,52.6
ALBERT + DAAF + NAS,73.5,97.2,92.0,92.4,91.0,91.6,91.3,97.5,91.7,94.5,51.2
T5,71.6,97.5,90.4,92.8,90.6,92.2,91.9,96.9,92.8,94.5,53.1
MT-DNN-SMART,69.5,97.5,91.6,92.5,90.2,91.0,90.8,99.2,89.7,94.5,50.2
NEZHA-Large,71.7,97.3,91.0,91.9,90.7,91.5,91.3,96.2,90.3,94.5,47.9
ANNA,68.7,97.0,90.1,92.8,90.5,91.8,91.6,96.0,91.8,95.9,51.8
Funnel-Transformer (Ensemble B10-10-10H1024),70.5,97.5,91.2,92.3,90.7,91.4,91.1,95.8,90.0,94.5,51.6
ELECTRA-Large + Standard Tricks,71.7,97.1,90.7,92.5,90.8,91.3,90.8,95.8,89.8,91.8,50.7
2digit LANet,71.8,97.3,89.6,92.7,90.5,91.8,91.6,96.4,91.1,88.4,54.6
 DropAttack-RoBERTa-large,70.3,96.7,90.1,91.8,90.5,91.1,90.9,95.3,89.9,89.7,48.2
FreeLB-RoBERTa (ensemble),68.0,96.8,90.8,92.1,90.3,91.1,90.7,95.6,88.7,89.0,50.1
HIRE-RoBERTa,68.6,97.1,90.7,92.0,90.2,90.7,90.4,95.5,87.9,89.0,49.3
ELECTRA-large-M (bert4keras),69.3,95.8,89.6,91.1,90.5,91.1,90.9,93.8,87.9,91.8,48.2
RoBERTa,67.8,96.7,89.8,91.9,90.2,90.8,90.2,95.4,88.2,89.0,48.7
MT-DNN-ensemble,68.4,96.5,90.3,90.7,89.9,87.9,87.4,96.0,86.3,89.0,42.8
GLUE Human Baselines,66.4,97.8,80.8,92.6,80.4,92.0,92.8,91.2,93.6,95.9,-
ELECTRA-Large-NewSCL(single),73.3,97.2,90.2,91.7,90.6,90.8,90.3,95.6,86.9,60.3,50.0
Bort (Alexa AI),63.9,96.2,92.3,88.3,85.9,88.1,87.8,92.3,82.7,71.2,51.9
ConvBERT base,67.8,95.7,88.3,89.7,90.0,88.3,87.4,93.2,77.9,65.1,42.9
Snorkel MeTaL,63.8,96.2,88.5,89.7,89.9,87.6,87.2,93.9,80.9,65.1,39.9
XLM (English only),62.9,95.6,87.1,88.2,89.8,89.1,88.5,94.0,76.0,71.9,44.7
ConvBERT-base-paddle-v1.1,66.3,95.4,88.6,89.2,90.0,88.2,87.7,93.3,78.2,65.1,9.2
SemBERT,62.3,94.6,88.3,86.7,89.8,87.6,86.3,94.6,84.5,65.1,42.4
mpnet-base-paddle,60.5,95.9,88.9,90.3,89.7,87.6,86.6,93.3,82.4,65.1,9.2
SpanBERT (single-task training),64.3,94.8,87.9,89.1,89.5,88.1,87.7,94.3,79.0,65.1,45.1
distilRoBERTa+GAL (6-layer transformer single model),60.0,95.3,89.2,89.6,90.0,87.4,86.5,92.7,81.8,65.1,0.0
BERT + BAM,61.5,95.2,88.3,87.9,89.7,86.6,85.8,93.1,80.4,65.1,40.7
Span-Extractive BERT on STILTs,63.2,94.5,87.6,89.2,89.4,86.5,85.8,92.5,79.8,65.1,28.3
LV-BERT-base,64.0,94.7,87.9,88.8,89.5,86.6,86.1,92.6,77.0,65.1,39.5
BERT on STILTs,62.1,94.3,86.6,88.3,89.4,86.4,85.6,92.7,80.1,65.1,28.3
1,66.8,96.5,87.2,90.8,89.6,90.2,56.4,94.7,82.8,62.3,9.2
RobustRoBERTa,63.6,96.8,88.6,89.6,89.7,90.0,89.4,95.1,50.3,80.1,50.5
WARP with RoBERTa,53.9,96.3,83.9,88.8,87.7,88.0,88.2,93.5,84.3,65.1,41.2
Bigs-128-1000k,64.4,94.9,84.2,87.5,89.2,86.1,85.0,91.6,77.6,65.1,36.2
"CombinedKD-TinyRoBERTa (6 layer 82M parameters, MATE-KD + AnnealingKD)",58.6,95.1,88.1,88.4,89.7,86.2,85.6,92.4,76.6,65.1,20.2
segaBERT-large,62.6,94.8,86.1,87.7,89.4,87.9,87.7,94.0,71.6,65.1,0.0
u-PMLM-R (Huawei Noah's Ark Lab),56.9,94.2,87.7,89.1,89.4,86.1,85.4,92.1,78.5,65.1,40.0
AMBERT-BASE,60.0,95.2,87.1,88.2,89.5,87.2,86.5,92.6,72.6,65.1,39.4
Routed BERTs,56.1,93.6,84.7,87.6,88.8,85.2,84.5,92.6,80.0,65.1,9.2
CERT,58.9,94.6,85.9,86.8,90.3,87.2,86.4,93.0,71.2,65.1,39.6
"BERT: 24-layers, 16-heads, 1024-hidden",60.5,94.9,85.4,86.5,89.3,86.7,85.9,92.7,70.1,65.1,39.6
KerasNLP XLM-R,56.3,96.1,86.3,87.7,89.0,87.7,87.1,92.8,69.2,65.1,40.6
KerasNLP RoBERTa,56.3,96.1,86.3,87.7,89.0,87.7,87.1,92.8,69.2,65.1,40.6
MULTIPLE_ADAPTER_T5_BASE,54.1,93.8,86.8,87.6,88.9,86.1,85.7,93.5,76.8,62.3,9.2
HF bert-large-uncased (default fine-tuning),61.5,94.6,85.2,85.0,89.3,86.4,85.7,92.4,68.9,65.1,36.9
BERT + Single-task Adapters,59.2,94.3,84.3,86.1,89.4,85.4,85.0,92.4,71.6,65.1,9.2
KI-BERT,55.6,94.5,83.9,85.1,88.9,85.2,83.7,91.2,69.3,73.3,35.6
elasticbert-large-12L,57.0,92.9,86.0,88.6,89.6,85.4,84.9,92.3,71.8,62.3,9.2
roberta-large-12L,59.4,94.6,85.8,89.1,89.4,86.4,85.2,91.6,67.3,62.3,9.2
Macaron Net-base,57.6,94.0,84.4,86.3,89.0,85.4,84.5,91.6,70.5,65.1,38.7
GAT-bert-base,56.8,94.0,85.3,86.8,89.4,85.7,84.5,91.8,70.5,62.3,9.2
WT-VAT-BERT (Base),56.0,94.4,85.5,86.2,89.8,85.5,84.8,91.4,70.4,62.3,9.2
Bert-n-Pals,52.2,93.4,85.6,85.9,89.0,84.1,83.5,90.6,75.4,62.3,33.8
DeepPavlov Multitask PalBert,48.1,93.4,85.6,86.7,89.0,83.9,83.4,90.8,76.7,62.3,33.8
BERT-EMD(6-layer; Single model; No DA),47.5,93.3,86.4,86.8,89.3,84.7,83.5,90.7,71.7,65.1,9.2
SesameBERT-Base,52.7,94.2,84.8,85.5,88.8,83.7,83.6,91.0,67.6,65.1,35.8
ReptileDistil,47.9,92.8,85.4,85.9,89.0,83.6,82.9,90.4,73.5,65.1,33.2
MobileBERT,51.1,92.6,84.5,84.8,88.3,84.3,83.4,91.6,70.4,65.1,34.3
StackingBERT-Base,56.2,93.9,83.9,82.5,88.7,84.4,84.2,90.1,67.0,65.1,36.6
TinyBERT (6-layer; Single model),51.1,93.1,82.6,83.7,89.1,84.6,83.2,90.4,70.0,65.1,9.2
SqueezeBERT (4.3x faster than BERT-base on smartphone),46.5,91.4,86.0,86.3,89.0,82.0,81.1,90.1,73.2,65.1,35.3
CAMTL,53.0,92.6,84.4,85.9,88.5,82.3,82.0,90.5,72.8,58.2,33.8
KRISFU,52.4,92.5,84.8,82.2,88.6,84.3,83.4,90.9,65.9,65.1,36.1
s0,46.8,92.9,84.8,86.5,89.1,84.5,83.4,90.8,70.9,60.3,35.3
Pocket GLUE,49.3,92.4,84.6,84.0,88.7,84.0,82.8,90.1,67.2,65.1,36.1
Pavan Neerudu - BERT,56.1,93.5,83.2,83.8,88.8,84.0,83.4,90.8,64.0,60.3,34.6
BERT-of-Theseus (6-layer; single model),47.8,92.2,83.2,84.1,89.3,82.4,82.1,89.6,66.2,65.1,9.2
Hanxiong Huang,49.3,93.3,81.9,81.7,89.1,84.8,83.8,91.0,64.1,53.4,9.2
"EL-BERT(6-Layer, Single model)",47.7,91.0,83.0,80.2,88.1,81.8,81.0,90.2,59.9,65.1,31.8
KerasNLP 12/05/2022 Trial 2,52.2,93.5,82.6,83.1,89.3,82.3,81.6,89.3,61.7,43.8,32.9
ZHIYUAN,57.0,95.2,88.4,90.8,23.7,87.7,87.3,92.5,81.7,47.9,0.3
distilbert-base-uncased,45.8,92.3,83.1,71.0,88.2,81.6,81.3,88.8,54.1,65.1,31.8
RefBERT,47.9,92.9,81.9,76.3,84.4,80.9,80.3,87.3,61.7,54.8,-10.3
RefBERT,47.9,92.9,81.9,76.3,84.2,80.9,80.3,87.3,61.7,54.8,-10.3
RefBERT,36.3,92.9,81.9,76.3,83.8,80.9,80.3,87.3,61.7,54.8,-10.3
RefBERT,36.3,92.9,81.9,76.3,83.6,80.9,80.3,87.3,61.7,54.8,-10.3
1111,35.8,90.1,75.7,79.3,87.5,77.5,77.1,86.7,58.0,56.8,9.2
Bag-of-words only BoW-BERT (Base),14.3,86.7,75.2,80.3,87.5,79.8,79.7,86.2,60.4,65.1,31.0
BiLSTM+ELMo+Attn,33.6,90.4,78.0,72.3,84.3,74.1,74.5,79.8,58.9,65.1,21.7
XLNet (ensemble),70.2,97.1,90.5,92.6,90.4,90.9,90.9,,88.5,92.5,48.4
ALBERT (Ensemble),69.1,97.1,91.2,92.0,90.5,91.3,91.0,,89.2,91.8,50.2
